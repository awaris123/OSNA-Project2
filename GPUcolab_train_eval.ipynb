{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPUcolab_train_eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6PSt2NZju8wygR3qysBAT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awaris123/OSNA-Project2/blob/main/GPUcolab_train_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzqnVHRi7UqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95fe580-613b-4928-ec76-33df0d1f466a"
      },
      "source": [
        "!pip3 install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/5a/6e41e8383913dd2ba923cdcd02be2e03911595f4d2f9de559ecbed80d2d3/sentence-transformers-0.3.9.tar.gz (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 24.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 40kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 51kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: transformers<3.6.0,>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.5.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.1.91)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.9.3)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.6.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers<3.6.0,>=3.1.0->sentence-transformers) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2020.11.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.9-cp36-none-any.whl size=101036 sha256=a7c1c357697693031192c2c71b614d5edabe0e20400081025179f95ce589747c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/89/43/f2f5bc00b03ef9724b0f6254a97eaf159a4c4ddc024b33e07a\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-0.3.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4L1qeF0qNSB"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import pandas as pd\n",
        "import pdb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weVtM5ej4Iu1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499578c0-527f-4127-86d0-24fcceffa24f"
      },
      "source": [
        "#    def __init__(self, df, y_data, dim=(768*2), batch_size=200, n_classes=3, shuffle=True):\n",
        "#from simple_classifier_model import SimpleClassifier\n",
        "!pip3 install transformers\n",
        "from xlnet import XLNet\n",
        "\n",
        "df=pd.read_csv(\"https://raw.githubusercontent.com/awaris123/OSNA-Project2/main/option1-data/train.csv\")\n",
        "label_encoder = LabelEncoder()\n",
        "targets = label_encoder.fit_transform(df['label'].values)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[['title1_en', 'title2_en']], targets, test_size=0.2)\n",
        "df_train = pd.DataFrame()\n",
        "\n",
        "params={'lr': 0.01, 'opt': 'adam', 'model': 'SimpleClassifier', 'loss': 'crossentropy', 'batch_size': 25000}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "3C5wzDyf96g7",
        "outputId": "e143a831-5621-44d0-ff08-082372384335"
      },
      "source": [
        "#for xlnet\n",
        "from data_generator import DataGenerator\n",
        "import pickle\n",
        "\n",
        "training_generator = DataGenerator(X_train, y_train, total_samples=y_train.shape[0], batch_size=params['batch_size'])\n",
        "validation_generator = DataGenerator(X_test, y_test, total_samples=y_test.shape[0], batch_size=params['batch_size'])\n",
        "xlnet1= XLNet()\n",
        "model = xlnet1.model\n",
        "model.summary()\n",
        "def get_inputs(tweets, tokenizer, max_len=120):\n",
        "    \"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n",
        "    inps = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in tweets]\n",
        "    inp_tok = np.array([a['input_ids'] for a in inps])\n",
        "    ids = np.array([a['attention_mask'] for a in inps])\n",
        "    segments = np.array([a['token_type_ids'] for a in inps])\n",
        "    return inp_tok, ids, segments\n",
        "\n",
        "inp_tok, ids, segments = get_inputs(df[['title1_en', 'title2_en']], xlnet_tokenizer)\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=params['lr'],\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "opt = keras.optimizers.Adam(learning_rate=params['lr'])\n",
        "\n",
        "model.compile(optimizer=opt, loss=keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])#, keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "'''\n",
        "model.fit_generator(generator=training_generator,\n",
        "                          validation_data=validation_generator,\n",
        "                          epochs=10,\n",
        "                          workers=8,\n",
        "                    use_multiprocessing=False,\n",
        "                    verbose=1)\n",
        "'''\n",
        "model.save(\"/content/drive/MyDrive/joint_embeddings/test/xlnet_ft_unbalanced\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at xlnet-large-cased were not used when initializing TFXLNetModel: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFXLNetModel were initialized from the model checkpoint at xlnet-large-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetModel for predictions without further training.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "word_inputs (InputLayer)     [(None, 128)]             0         \n",
            "_________________________________________________________________\n",
            "tfxl_net_model_7 (TFXLNetMod ((None, 128, 1024),)      360268800 \n",
            "_________________________________________________________________\n",
            "tf_op_layer_strided_slice_7  [(None, 1, 1024)]         0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Squeeze_7 (Tenso [(None, 1024)]            0         \n",
            "_________________________________________________________________\n",
            "dropout_591 (Dropout)        (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 3075      \n",
            "=================================================================\n",
            "Total params: 360,271,875\n",
            "Trainable params: 360,271,875\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5733d9a0d586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minp_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title1_en'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlnet1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n\u001b[1;32m     20\u001b[0m     \u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqVPx4OzQKtL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e41e6e9-ad0b-44a8-9ebb-d1efa5e3bd57"
      },
      "source": [
        "from data_generator import DataGenerator\n",
        "import pickle\n",
        "\n",
        "training_generator = DataGenerator(X_train, y_train, total_samples=y_train.shape[0], batch_size=params['batch_size'])\n",
        "validation_generator = DataGenerator(X_test, y_test, total_samples=y_test.shape[0], batch_size=params['batch_size'])\n",
        "all_generator = DataGenerator(df, targets, total_samples=targets.shape[0], batch_size=params['batch_size'])\n",
        "joint_embedding = np.empty((all_generator.total_samples, *all_generator.dim))\n",
        "y = np.empty((all_generator.total_samples), dtype=int)\n",
        "\n",
        "for i in range(int(all_generator.total_samples/all_generator.batch_size)):\n",
        "  #joint_embedding[i*all_generator.batch_size:(i+1)*all_generator.batch_size,:], y[i*all_generator.batch_size:(i+1)*all_generator.batch_size]=all_generator.__getitem__(i) #\n",
        "  e1, y1=all_generator.__getitem__(i) #\n",
        "\n",
        "  with open(f\"./joint_embedding_{i}\", 'wb') as f:\n",
        "    pickle.dump(e1, f)\n",
        "\n",
        "  with open(f\"./y_{i}\", 'wb') as f:\n",
        "    pickle.dump(y1, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 / 256442\n",
            "25000 / 256442\n",
            "50000 / 256442\n",
            "75000 / 256442\n",
            "100000 / 256442\n",
            "125000 / 256442\n",
            "150000 / 256442\n",
            "175000 / 256442\n",
            "200000 / 256442\n",
            "225000 / 256442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "342LR-SSQrye",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "fc36bf40-0543-4ee0-d537-c502d840b9f1"
      },
      "source": [
        "del df\n",
        "del targets\n",
        "import pickle\n",
        "with open(\"./joint_embedding\", 'wb') as f:\n",
        "  pickle.dump(joint_embedding, f)\n",
        "\n",
        "with open(\"./y\", 'wb') as f:\n",
        "  pickle.dump(y, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-84d7b1424f29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./joint_embedding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'joint_embedding' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBoWjxtTbheW",
        "outputId": "f264d850-74f4-41d3-cb04-07fa7e0843a6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXILmi2DSv4K"
      },
      "source": [
        "mkdir ./drive/MyDrive/joint_embeddings\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeN_3P-aOlA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52bd834-2d50-408b-e8c4-3a888b8359e7"
      },
      "source": [
        "!cp -r joint_embedding ./drive/MyDrive/joint_embeddings/\n",
        "!cp -r y ./drive/MyDrive/joint_embeddings/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'joint_embedding': No such file or directory\n",
            "cp: cannot stat 'y': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "nvUJN-jLO4OP",
        "outputId": "fd347d5b-fb33-4a16-c6ea-e703869382d3"
      },
      "source": [
        "import pickle\n",
        "path=\"./drive/MyDrive/joint_embeddings/\"\n",
        "batch_size=25000\n",
        "X_all = np.empty((batch_size*10, 1536))\n",
        "y_all = np.empty((batch_size*10))\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "  with open(f\"{path}/joint_embedding_{i}\", 'rb') as f:\n",
        "    X=pickle.load(f)\n",
        "    X_all[i*batch_size:(i+1)*batch_size,:]=X\n",
        "\n",
        "  with open(f\"{path}/y_{i}\", 'rb') as f:\n",
        "    y=pickle.load(f)\n",
        "    y_all[i*batch_size:(i+1)*batch_size]=y\n",
        "  print(i)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-aa590bde916a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path}/joint_embedding_{i}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mX_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyzaQTjZRVM1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "91f12f1d-16c1-4b32-8fa0-a5c0a69b32d6"
      },
      "source": [
        "with open(\"./joint_embedding\", 'wb') as f:\n",
        "  pickle.dump(X_all, f)\n",
        "\n",
        "with open(\"./y\", 'wb') as f:\n",
        "  pickle.dump(y_all, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-165bb3fe84b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./joint_embedding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRKyD7hsRX3j"
      },
      "source": [
        "#load\n",
        "import pickle\n",
        "path=\"./drive/MyDrive/joint_embeddings/\"\n",
        "\n",
        "with open(f\"{path}joint_embedding\", 'rb') as f:\n",
        "  X_all=pickle.load(f)\n",
        "\n",
        "with open(f\"{path}y\", 'rb') as f:\n",
        "  y_all=pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukdafiQLTSk1"
      },
      "source": [
        "!cp /content/drive/MyDrive/joint_embeddings/data_generator.py .\n",
        "!cp /content/drive/MyDrive/joint_embeddings/simple_classifier_model.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkPWHBOiuO1P"
      },
      "source": [
        "#train from preprocessed data\n",
        "from data_generator import DataGenerator\n",
        "import pickle\n",
        "\n",
        "params={'lr': 0.001, 'opt': 'adam', 'model': 'SimpleClassifier', 'loss': 'crossentropy', 'batch_size': 128}\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
        "del X_all\n",
        "del y_all\n",
        "training_generator = DataGenerator(X_train, y_train, total_samples=y_train.shape[0], batch_size=params['batch_size'])\n",
        "validation_generator = DataGenerator(X_test, y_test, total_samples=y_test.shape[0], batch_size=params['batch_size'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0u0HK2svGHP",
        "outputId": "8e38ccd1-3249-4c0b-9f46-c1b4c2b23096"
      },
      "source": [
        "import simple_classifier_model #import SimpleClassifier\n",
        "import importlib\n",
        "importlib.reload(simple_classifier_model)\n",
        "\n",
        "model = simple_classifier_model.SimpleClassifier(input_shape=768*2)\n",
        "model.model.summary()\n",
        "\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=params['lr'],\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "opt = keras.optimizers.Adam(learning_rate=params['lr'])\n",
        "\n",
        "model.model.compile(optimizer=opt, loss=keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])#, keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "model.model.fit_generator(generator=training_generator,\n",
        "                          validation_data=validation_generator,\n",
        "                          epochs=100,\n",
        "                          workers=8,\n",
        "                    use_multiprocessing=False,\n",
        "                    verbose=1)\n",
        "model.model.save(\"/content/drive/MyDrive/joint_embeddings/test/bert_mlp_unbalanced_1500_512_bs128_leakyrelu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 1500)              2305500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 1500)              6000      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 1500)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 250)               375250    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 250)               1000      \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 50)                12550     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 2,700,653\n",
            "Trainable params: 2,697,053\n",
            "Non-trainable params: 3,600\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.5673 - accuracy: 0.7434 - val_loss: 0.4723 - val_accuracy: 0.7846\n",
            "Epoch 2/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.4718 - accuracy: 0.7885 - val_loss: 0.4473 - val_accuracy: 0.7997\n",
            "Epoch 3/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.4402 - accuracy: 0.8049 - val_loss: 0.4275 - val_accuracy: 0.8096\n",
            "Epoch 4/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.4153 - accuracy: 0.8170 - val_loss: 0.4190 - val_accuracy: 0.8150\n",
            "Epoch 5/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.3941 - accuracy: 0.8270 - val_loss: 0.4059 - val_accuracy: 0.8208\n",
            "Epoch 6/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.3773 - accuracy: 0.8362 - val_loss: 0.3943 - val_accuracy: 0.8274\n",
            "Epoch 7/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.3584 - accuracy: 0.8452 - val_loss: 0.3887 - val_accuracy: 0.8310\n",
            "Epoch 8/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.3441 - accuracy: 0.8509 - val_loss: 0.3889 - val_accuracy: 0.8328\n",
            "Epoch 9/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.3293 - accuracy: 0.8575 - val_loss: 0.3840 - val_accuracy: 0.8351\n",
            "Epoch 10/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.3145 - accuracy: 0.8653 - val_loss: 0.3857 - val_accuracy: 0.8360\n",
            "Epoch 11/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.3010 - accuracy: 0.8716 - val_loss: 0.3842 - val_accuracy: 0.8378\n",
            "Epoch 12/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.2885 - accuracy: 0.8772 - val_loss: 0.3848 - val_accuracy: 0.8373\n",
            "Epoch 13/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.2763 - accuracy: 0.8826 - val_loss: 0.3959 - val_accuracy: 0.8386\n",
            "Epoch 14/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.2648 - accuracy: 0.8886 - val_loss: 0.4061 - val_accuracy: 0.8360\n",
            "Epoch 15/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.2542 - accuracy: 0.8934 - val_loss: 0.4007 - val_accuracy: 0.8363\n",
            "Epoch 16/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.2447 - accuracy: 0.8974 - val_loss: 0.4000 - val_accuracy: 0.8403\n",
            "Epoch 17/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.2340 - accuracy: 0.9024 - val_loss: 0.4021 - val_accuracy: 0.8365\n",
            "Epoch 18/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.2257 - accuracy: 0.9058 - val_loss: 0.4112 - val_accuracy: 0.8457\n",
            "Epoch 19/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.2167 - accuracy: 0.9104 - val_loss: 0.4131 - val_accuracy: 0.8443\n",
            "Epoch 20/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.2099 - accuracy: 0.9128 - val_loss: 0.4224 - val_accuracy: 0.8443\n",
            "Epoch 21/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.2016 - accuracy: 0.9175 - val_loss: 0.4575 - val_accuracy: 0.8391\n",
            "Epoch 22/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1950 - accuracy: 0.9208 - val_loss: 0.4269 - val_accuracy: 0.8455\n",
            "Epoch 23/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.1878 - accuracy: 0.9232 - val_loss: 0.4294 - val_accuracy: 0.8476\n",
            "Epoch 24/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.1826 - accuracy: 0.9257 - val_loss: 0.4418 - val_accuracy: 0.8430\n",
            "Epoch 25/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1752 - accuracy: 0.9285 - val_loss: 0.4415 - val_accuracy: 0.8479\n",
            "Epoch 26/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1722 - accuracy: 0.9302 - val_loss: 0.4746 - val_accuracy: 0.8443\n",
            "Epoch 27/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1657 - accuracy: 0.9337 - val_loss: 0.4612 - val_accuracy: 0.8467\n",
            "Epoch 28/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1623 - accuracy: 0.9344 - val_loss: 0.4485 - val_accuracy: 0.8486\n",
            "Epoch 29/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1561 - accuracy: 0.9376 - val_loss: 0.4639 - val_accuracy: 0.8436\n",
            "Epoch 30/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.1527 - accuracy: 0.9392 - val_loss: 0.4806 - val_accuracy: 0.8436\n",
            "Epoch 31/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1471 - accuracy: 0.9414 - val_loss: 0.4863 - val_accuracy: 0.8462\n",
            "Epoch 32/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1445 - accuracy: 0.9432 - val_loss: 0.4752 - val_accuracy: 0.8490\n",
            "Epoch 33/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1407 - accuracy: 0.9444 - val_loss: 0.4905 - val_accuracy: 0.8499\n",
            "Epoch 34/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1367 - accuracy: 0.9464 - val_loss: 0.4846 - val_accuracy: 0.8467\n",
            "Epoch 35/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1346 - accuracy: 0.9473 - val_loss: 0.4792 - val_accuracy: 0.8500\n",
            "Epoch 36/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1307 - accuracy: 0.9487 - val_loss: 0.4984 - val_accuracy: 0.8492\n",
            "Epoch 37/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1285 - accuracy: 0.9496 - val_loss: 0.5104 - val_accuracy: 0.8498\n",
            "Epoch 38/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1251 - accuracy: 0.9515 - val_loss: 0.5172 - val_accuracy: 0.8514\n",
            "Epoch 39/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1232 - accuracy: 0.9522 - val_loss: 0.4983 - val_accuracy: 0.8510\n",
            "Epoch 40/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1195 - accuracy: 0.9537 - val_loss: 0.5225 - val_accuracy: 0.8489\n",
            "Epoch 41/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1166 - accuracy: 0.9551 - val_loss: 0.5222 - val_accuracy: 0.8508\n",
            "Epoch 42/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.1145 - accuracy: 0.9552 - val_loss: 0.5114 - val_accuracy: 0.8483\n",
            "Epoch 43/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1130 - accuracy: 0.9564 - val_loss: 0.5212 - val_accuracy: 0.8478\n",
            "Epoch 44/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.1097 - accuracy: 0.9579 - val_loss: 0.5333 - val_accuracy: 0.8511\n",
            "Epoch 45/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1096 - accuracy: 0.9579 - val_loss: 0.5346 - val_accuracy: 0.8494\n",
            "Epoch 46/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1051 - accuracy: 0.9599 - val_loss: 0.5585 - val_accuracy: 0.8476\n",
            "Epoch 47/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1045 - accuracy: 0.9599 - val_loss: 0.5425 - val_accuracy: 0.8477\n",
            "Epoch 48/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.1029 - accuracy: 0.9605 - val_loss: 0.5587 - val_accuracy: 0.8486\n",
            "Epoch 49/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1015 - accuracy: 0.9612 - val_loss: 0.5481 - val_accuracy: 0.8521\n",
            "Epoch 50/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1007 - accuracy: 0.9613 - val_loss: 0.5449 - val_accuracy: 0.8540\n",
            "Epoch 51/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0981 - accuracy: 0.9626 - val_loss: 0.5739 - val_accuracy: 0.8475\n",
            "Epoch 52/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0944 - accuracy: 0.9637 - val_loss: 0.5578 - val_accuracy: 0.8514\n",
            "Epoch 53/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.0937 - accuracy: 0.9643 - val_loss: 0.5612 - val_accuracy: 0.8520\n",
            "Epoch 54/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.0927 - accuracy: 0.9646 - val_loss: 0.5687 - val_accuracy: 0.8526\n",
            "Epoch 55/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.0919 - accuracy: 0.9652 - val_loss: 0.5791 - val_accuracy: 0.8510\n",
            "Epoch 56/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0896 - accuracy: 0.9661 - val_loss: 0.5827 - val_accuracy: 0.8521\n",
            "Epoch 57/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0886 - accuracy: 0.9669 - val_loss: 0.5772 - val_accuracy: 0.8533\n",
            "Epoch 58/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0872 - accuracy: 0.9669 - val_loss: 0.5907 - val_accuracy: 0.8544\n",
            "Epoch 59/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0865 - accuracy: 0.9672 - val_loss: 0.5839 - val_accuracy: 0.8518\n",
            "Epoch 60/100\n",
            "3125/3125 [==============================] - 19s 6ms/step - loss: 0.0848 - accuracy: 0.9677 - val_loss: 0.5948 - val_accuracy: 0.8518\n",
            "Epoch 61/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0829 - accuracy: 0.9685 - val_loss: 0.6022 - val_accuracy: 0.8519\n",
            "Epoch 62/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0821 - accuracy: 0.9687 - val_loss: 0.5989 - val_accuracy: 0.8519\n",
            "Epoch 63/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0816 - accuracy: 0.9693 - val_loss: 0.6047 - val_accuracy: 0.8519\n",
            "Epoch 64/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0810 - accuracy: 0.9698 - val_loss: 0.6051 - val_accuracy: 0.8520\n",
            "Epoch 65/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0802 - accuracy: 0.9704 - val_loss: 0.6179 - val_accuracy: 0.8539\n",
            "Epoch 66/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0782 - accuracy: 0.9711 - val_loss: 0.6083 - val_accuracy: 0.8516\n",
            "Epoch 67/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0769 - accuracy: 0.9710 - val_loss: 0.6626 - val_accuracy: 0.8495\n",
            "Epoch 68/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0767 - accuracy: 0.9716 - val_loss: 0.6109 - val_accuracy: 0.8521\n",
            "Epoch 69/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0745 - accuracy: 0.9721 - val_loss: 0.6028 - val_accuracy: 0.8526\n",
            "Epoch 70/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0751 - accuracy: 0.9719 - val_loss: 0.6163 - val_accuracy: 0.8549\n",
            "Epoch 71/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0726 - accuracy: 0.9730 - val_loss: 0.6209 - val_accuracy: 0.8529\n",
            "Epoch 72/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0716 - accuracy: 0.9736 - val_loss: 0.6307 - val_accuracy: 0.8531\n",
            "Epoch 73/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0714 - accuracy: 0.9735 - val_loss: 0.6361 - val_accuracy: 0.8552\n",
            "Epoch 74/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0715 - accuracy: 0.9733 - val_loss: 0.6353 - val_accuracy: 0.8531\n",
            "Epoch 75/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0698 - accuracy: 0.9741 - val_loss: 0.6437 - val_accuracy: 0.8509\n",
            "Epoch 76/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0688 - accuracy: 0.9743 - val_loss: 0.6430 - val_accuracy: 0.8512\n",
            "Epoch 77/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0687 - accuracy: 0.9743 - val_loss: 0.6537 - val_accuracy: 0.8534\n",
            "Epoch 78/100\n",
            "3125/3125 [==============================] - 19s 6ms/step - loss: 0.0674 - accuracy: 0.9751 - val_loss: 0.6546 - val_accuracy: 0.8512\n",
            "Epoch 79/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0672 - accuracy: 0.9750 - val_loss: 0.6557 - val_accuracy: 0.8553\n",
            "Epoch 80/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0666 - accuracy: 0.9756 - val_loss: 0.6730 - val_accuracy: 0.8547\n",
            "Epoch 81/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0656 - accuracy: 0.9755 - val_loss: 0.6622 - val_accuracy: 0.8539\n",
            "Epoch 82/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0657 - accuracy: 0.9758 - val_loss: 0.6565 - val_accuracy: 0.8528\n",
            "Epoch 83/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0642 - accuracy: 0.9761 - val_loss: 0.6222 - val_accuracy: 0.8554\n",
            "Epoch 84/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0641 - accuracy: 0.9761 - val_loss: 0.6860 - val_accuracy: 0.8526\n",
            "Epoch 85/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0626 - accuracy: 0.9774 - val_loss: 0.6642 - val_accuracy: 0.8549\n",
            "Epoch 86/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0622 - accuracy: 0.9769 - val_loss: 0.6700 - val_accuracy: 0.8551\n",
            "Epoch 87/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0615 - accuracy: 0.9774 - val_loss: 0.6874 - val_accuracy: 0.8501\n",
            "Epoch 88/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0626 - accuracy: 0.9769 - val_loss: 0.6276 - val_accuracy: 0.8535\n",
            "Epoch 89/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0606 - accuracy: 0.9782 - val_loss: 0.6828 - val_accuracy: 0.8555\n",
            "Epoch 90/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0604 - accuracy: 0.9778 - val_loss: 0.6601 - val_accuracy: 0.8531\n",
            "Epoch 91/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0595 - accuracy: 0.9780 - val_loss: 0.6957 - val_accuracy: 0.8554\n",
            "Epoch 92/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0592 - accuracy: 0.9787 - val_loss: 0.6528 - val_accuracy: 0.8554\n",
            "Epoch 93/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0591 - accuracy: 0.9784 - val_loss: 0.6777 - val_accuracy: 0.8535\n",
            "Epoch 94/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0583 - accuracy: 0.9790 - val_loss: 0.6875 - val_accuracy: 0.8542\n",
            "Epoch 95/100\n",
            "3125/3125 [==============================] - 19s 6ms/step - loss: 0.0589 - accuracy: 0.9784 - val_loss: 0.6553 - val_accuracy: 0.8558\n",
            "Epoch 96/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0568 - accuracy: 0.9791 - val_loss: 0.6958 - val_accuracy: 0.8545\n",
            "Epoch 97/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0567 - accuracy: 0.9794 - val_loss: 0.6843 - val_accuracy: 0.8562\n",
            "Epoch 98/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0557 - accuracy: 0.9795 - val_loss: 0.7055 - val_accuracy: 0.8535\n",
            "Epoch 99/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0551 - accuracy: 0.9803 - val_loss: 0.6878 - val_accuracy: 0.8519\n",
            "Epoch 100/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0555 - accuracy: 0.9798 - val_loss: 0.7053 - val_accuracy: 0.8561\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/joint_embeddings/test/bert_mlp_unbalanced_1500_512_bs128_leakyrelu/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/joint_embeddings/test/bert_mlp_unbalanced_1500_512_bs128_leakyrelu/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M-rYge_1CQN",
        "outputId": "a74424ee-821b-476b-f214-4b5829a45fee"
      },
      "source": [
        "# EVALUATION\n",
        "import data_generator #import DataGenerator\n",
        "importlib.reload(data_generator)\n",
        "\n",
        "#model=keras.models.load(\"/content/drive/MyDrive/joint_embeddings/test/bert_mlp_unbalanced_1500\")\n",
        "df=pd.read_csv(\"https://raw.githubusercontent.com/awaris123/OSNA-Project2/main/option1-data/train.csv\")\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit_transform(df['label'].values)\n",
        "\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/joint_embeddings/test.csv\", sep=',')\n",
        "final_df=pd.DataFrame(data=None, columns=['id', 'label'])\n",
        "final_df['id'] = df['id']\n",
        "targets=np.empty((final_df.shape[0]), dtype=int)\n",
        "all_generator = data_generator.DataGenerator(df, targets, n_classes=3, total_samples=targets.shape[0], batch_size=128, pretrained_embeddings=False)\n",
        "print(int(all_generator.total_samples/all_generator.batch_size))\n",
        "for i in range(int(all_generator.total_samples/all_generator.batch_size)):\n",
        "  #joint_embedding[i*all_generator.batch_size:(i+1)*all_generator.batch_size,:], y[i*all_generator.batch_size:(i+1)*all_generator.batch_size]=all_generator.__getitem__(i) #\n",
        "  print(i, end = ' ')\n",
        "  e1, _=all_generator.__getitem__(i) #\n",
        "  targets[i*all_generator.batch_size:(i+1)*all_generator.batch_size]=np.argmax(model.model.predict(e1), axis=1)\n",
        "  print(i, end = ' ')\n",
        "\n",
        "\n",
        "#label decoder\n",
        "#final_df['label'] = encoder.inverse_transform(targets)\n",
        "\n",
        "#convert to csv\n",
        "#final_df.to_csv(\"/content/drive/MyDrive/joint_embeddings/sample_submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 11 11 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 29 30 30 31 31 32 32 33 33 34 34 35 35 36 36 37 37 38 38 39 39 40 40 41 41 42 42 43 43 44 44 45 45 46 46 47 47 48 48 49 49 50 50 51 51 52 52 53 53 54 54 55 55 56 56 57 57 58 58 59 59 60 60 61 61 62 62 63 63 64 64 65 65 66 66 67 67 68 68 69 69 70 70 71 71 72 72 73 73 74 74 75 75 76 76 77 77 78 78 79 79 80 80 81 81 82 82 83 83 84 84 85 85 86 86 87 87 88 88 89 89 90 90 91 91 92 92 93 93 94 94 95 95 96 96 97 97 98 98 99 99 100 100 101 101 102 102 103 103 104 104 105 105 106 106 107 107 108 108 109 109 110 110 111 111 112 112 113 113 114 114 115 115 116 116 117 117 118 118 119 119 120 120 121 121 122 122 123 123 124 124 125 125 126 126 127 127 128 128 129 129 130 130 131 131 132 132 133 133 134 134 135 135 136 136 137 137 138 138 139 139 140 140 141 141 142 142 143 143 144 144 145 145 146 146 147 147 148 148 149 149 150 150 151 151 152 152 153 153 154 154 155 155 156 156 157 157 158 158 159 159 160 160 161 161 162 162 163 163 164 164 165 165 166 166 167 167 168 168 169 169 170 170 171 171 172 172 173 173 174 174 175 175 176 176 177 177 178 178 179 179 180 180 181 181 182 182 183 183 184 184 185 185 186 186 187 187 188 188 189 189 190 190 191 191 192 192 193 193 194 194 195 195 196 196 197 197 198 198 199 199 200 200 201 201 202 202 203 203 204 204 205 205 206 206 207 207 208 208 209 209 210 210 211 211 212 212 213 213 214 214 215 215 216 216 217 217 218 218 219 219 220 220 221 221 222 222 223 223 224 224 225 225 226 226 227 227 228 228 229 229 230 230 231 231 232 232 233 233 234 234 235 235 236 236 237 237 238 238 239 239 240 240 241 241 242 242 243 243 244 244 245 245 246 246 247 247 248 248 249 249 250 250 251 251 252 252 253 253 254 254 255 255 256 256 257 257 258 258 259 259 260 260 261 261 262 262 263 263 264 264 265 265 266 266 267 267 268 268 269 269 270 270 271 271 272 272 273 273 274 274 275 275 276 276 277 277 278 278 279 279 280 280 281 281 282 282 283 283 284 284 285 285 286 286 287 287 288 288 289 289 290 290 291 291 292 292 293 293 294 294 295 295 296 296 297 297 298 298 299 299 300 300 301 301 302 302 303 303 304 304 305 305 306 306 307 307 308 308 309 309 310 310 311 311 312 312 313 313 314 314 315 315 316 316 317 317 318 318 319 319 320 320 321 321 322 322 323 323 324 324 325 325 326 326 327 327 328 328 329 329 330 330 331 331 332 332 333 333 334 334 335 335 336 336 337 337 338 338 339 339 340 340 341 341 342 342 343 343 344 344 345 345 346 346 347 347 348 348 349 349 350 350 351 351 352 352 353 353 354 354 355 355 356 356 357 357 358 358 359 359 360 360 361 361 362 362 363 363 364 364 365 365 366 366 367 367 368 368 369 369 370 370 371 371 372 372 373 373 374 374 375 375 376 376 377 377 378 378 379 379 380 380 381 381 382 382 383 383 384 384 385 385 386 386 387 387 388 388 389 389 390 390 391 391 392 392 393 393 394 394 395 395 396 396 397 397 398 398 399 399 400 400 401 401 402 402 403 403 404 404 405 405 406 406 407 407 408 408 409 409 410 410 411 411 412 412 413 413 414 414 415 415 416 416 417 417 418 418 419 419 420 420 421 421 422 422 423 423 424 424 425 425 426 426 427 427 428 428 429 429 430 430 431 431 432 432 433 433 434 434 435 435 436 436 437 437 438 438 439 439 440 440 441 441 442 442 443 443 444 444 445 445 446 446 447 447 448 448 449 449 450 450 451 451 452 452 453 453 454 454 455 455 456 456 457 457 458 458 459 459 460 460 461 461 462 462 463 463 464 464 465 465 466 466 467 467 468 468 469 469 470 470 471 471 472 472 473 473 474 474 475 475 476 476 477 477 478 478 479 479 480 480 481 481 482 482 483 483 484 484 485 485 486 486 487 487 488 488 489 489 490 490 491 491 492 492 493 493 494 494 495 495 496 496 497 497 498 498 499 499 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVSRTmWr-A6_"
      },
      "source": [
        "len([i for i in targets if i > 2])\n",
        "le_name_mapping = dict(zip(encoder.transform(encoder.classes_), encoder.classes_))\n",
        "le_name_mapping\n",
        "final_df['label']=[le_name_mapping.get(i, 'unrelated') for i in targets]\n",
        "\n",
        "#label decoder\n",
        "\n",
        "#convert to csv\n",
        "final_df.to_csv(\"/content/drive/MyDrive/joint_embeddings/sample_submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jhXX9SBArJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32c5d62-157b-4caf-efdd-7ec786d4b5d5"
      },
      "source": [
        "training_generator = DataGenerator(X_train, y_train, total_samples=y_train.shape[0], batch_size=64)\n",
        "validation_generator = DataGenerator(X_test, y_test, total_samples=y_test.shape[0], batch_size=params['batch_size'])\n",
        "model.model.fit_generator(generator=training_generator,\n",
        "                          validation_data=validation_generator,\n",
        "                          epochs=100,\n",
        "                          workers=8,\n",
        "                    use_multiprocessing=False,\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1538 - accuracy: 0.9401 - val_loss: 0.5010 - val_accuracy: 0.8488\n",
            "Epoch 2/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1452 - accuracy: 0.9431 - val_loss: 0.4879 - val_accuracy: 0.8515\n",
            "Epoch 3/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1438 - accuracy: 0.9435 - val_loss: 0.4871 - val_accuracy: 0.8535\n",
            "Epoch 4/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1415 - accuracy: 0.9448 - val_loss: 0.4845 - val_accuracy: 0.8544\n",
            "Epoch 5/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1412 - accuracy: 0.9445 - val_loss: 0.4968 - val_accuracy: 0.8523\n",
            "Epoch 6/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1395 - accuracy: 0.9455 - val_loss: 0.4625 - val_accuracy: 0.8549\n",
            "Epoch 7/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1397 - accuracy: 0.9449 - val_loss: 0.4834 - val_accuracy: 0.8540\n",
            "Epoch 8/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1371 - accuracy: 0.9464 - val_loss: 0.4885 - val_accuracy: 0.8567\n",
            "Epoch 9/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1346 - accuracy: 0.9475 - val_loss: 0.4740 - val_accuracy: 0.8543\n",
            "Epoch 10/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1345 - accuracy: 0.9476 - val_loss: 0.5016 - val_accuracy: 0.8471\n",
            "Epoch 11/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1339 - accuracy: 0.9481 - val_loss: 0.5190 - val_accuracy: 0.8479\n",
            "Epoch 12/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1329 - accuracy: 0.9481 - val_loss: 0.4786 - val_accuracy: 0.8540\n",
            "Epoch 13/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1318 - accuracy: 0.9487 - val_loss: 0.4789 - val_accuracy: 0.8569\n",
            "Epoch 14/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1328 - accuracy: 0.9486 - val_loss: 0.5362 - val_accuracy: 0.8531\n",
            "Epoch 15/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1288 - accuracy: 0.9503 - val_loss: 0.4906 - val_accuracy: 0.8549\n",
            "Epoch 16/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1285 - accuracy: 0.9498 - val_loss: 0.5295 - val_accuracy: 0.8468\n",
            "Epoch 17/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1286 - accuracy: 0.9496 - val_loss: 0.4847 - val_accuracy: 0.8543\n",
            "Epoch 18/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.1277 - accuracy: 0.9506 - val_loss: 0.4946 - val_accuracy: 0.8579\n",
            "Epoch 19/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1261 - accuracy: 0.9507 - val_loss: 0.4944 - val_accuracy: 0.8563\n",
            "Epoch 20/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1235 - accuracy: 0.9526 - val_loss: 0.4902 - val_accuracy: 0.8568\n",
            "Epoch 21/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1255 - accuracy: 0.9510 - val_loss: 0.5010 - val_accuracy: 0.8561\n",
            "Epoch 22/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1223 - accuracy: 0.9528 - val_loss: 0.5244 - val_accuracy: 0.8510\n",
            "Epoch 23/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1216 - accuracy: 0.9533 - val_loss: 0.5321 - val_accuracy: 0.8476\n",
            "Epoch 24/100\n",
            "3125/3125 [==============================] - 16s 5ms/step - loss: 0.1212 - accuracy: 0.9529 - val_loss: 0.5003 - val_accuracy: 0.8553\n",
            "Epoch 25/100\n",
            "3125/3125 [==============================] - 16s 5ms/step - loss: 0.1201 - accuracy: 0.9535 - val_loss: 0.5586 - val_accuracy: 0.8486\n",
            "Epoch 26/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.1200 - accuracy: 0.9535 - val_loss: 0.5040 - val_accuracy: 0.8556\n",
            "Epoch 27/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1186 - accuracy: 0.9544 - val_loss: 0.5000 - val_accuracy: 0.8555\n",
            "Epoch 28/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1174 - accuracy: 0.9542 - val_loss: 0.5314 - val_accuracy: 0.8563\n",
            "Epoch 29/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1169 - accuracy: 0.9546 - val_loss: 0.5133 - val_accuracy: 0.8542\n",
            "Epoch 30/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1179 - accuracy: 0.9548 - val_loss: 0.5242 - val_accuracy: 0.8515\n",
            "Epoch 31/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1145 - accuracy: 0.9564 - val_loss: 0.5173 - val_accuracy: 0.8528\n",
            "Epoch 32/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1144 - accuracy: 0.9559 - val_loss: 0.5130 - val_accuracy: 0.8580\n",
            "Epoch 33/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1155 - accuracy: 0.9555 - val_loss: 0.4964 - val_accuracy: 0.8576\n",
            "Epoch 34/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1128 - accuracy: 0.9565 - val_loss: 0.5212 - val_accuracy: 0.8545\n",
            "Epoch 35/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1126 - accuracy: 0.9569 - val_loss: 0.5066 - val_accuracy: 0.8575\n",
            "Epoch 36/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.1132 - accuracy: 0.9564 - val_loss: 0.5107 - val_accuracy: 0.8553\n",
            "Epoch 37/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1116 - accuracy: 0.9571 - val_loss: 0.5278 - val_accuracy: 0.8536\n",
            "Epoch 38/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1104 - accuracy: 0.9575 - val_loss: 0.5277 - val_accuracy: 0.8524\n",
            "Epoch 39/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1099 - accuracy: 0.9577 - val_loss: 0.5710 - val_accuracy: 0.8509\n",
            "Epoch 40/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1094 - accuracy: 0.9579 - val_loss: 0.5736 - val_accuracy: 0.8467\n",
            "Epoch 41/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1078 - accuracy: 0.9590 - val_loss: 0.5273 - val_accuracy: 0.8535\n",
            "Epoch 42/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1084 - accuracy: 0.9583 - val_loss: 0.5465 - val_accuracy: 0.8558\n",
            "Epoch 43/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1072 - accuracy: 0.9594 - val_loss: 0.5441 - val_accuracy: 0.8539\n",
            "Epoch 44/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1075 - accuracy: 0.9590 - val_loss: 0.5250 - val_accuracy: 0.8536\n",
            "Epoch 45/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1062 - accuracy: 0.9596 - val_loss: 0.5427 - val_accuracy: 0.8561\n",
            "Epoch 46/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.1042 - accuracy: 0.9598 - val_loss: 0.5296 - val_accuracy: 0.8575\n",
            "Epoch 47/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1055 - accuracy: 0.9598 - val_loss: 0.5286 - val_accuracy: 0.8567\n",
            "Epoch 48/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1033 - accuracy: 0.9603 - val_loss: 0.6557 - val_accuracy: 0.8327\n",
            "Epoch 49/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1036 - accuracy: 0.9605 - val_loss: 0.5445 - val_accuracy: 0.8569\n",
            "Epoch 50/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1025 - accuracy: 0.9609 - val_loss: 0.5570 - val_accuracy: 0.8586\n",
            "Epoch 51/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1025 - accuracy: 0.9610 - val_loss: 0.5741 - val_accuracy: 0.8532\n",
            "Epoch 52/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1036 - accuracy: 0.9604 - val_loss: 0.5771 - val_accuracy: 0.8493\n",
            "Epoch 53/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.1013 - accuracy: 0.9613 - val_loss: 0.5406 - val_accuracy: 0.8564\n",
            "Epoch 54/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.1018 - accuracy: 0.9608 - val_loss: 0.5298 - val_accuracy: 0.8570\n",
            "Epoch 55/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.1015 - accuracy: 0.9613 - val_loss: 0.5433 - val_accuracy: 0.8559\n",
            "Epoch 56/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0998 - accuracy: 0.9619 - val_loss: 0.6161 - val_accuracy: 0.8419\n",
            "Epoch 57/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0990 - accuracy: 0.9626 - val_loss: 0.5960 - val_accuracy: 0.8588\n",
            "Epoch 58/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0990 - accuracy: 0.9624 - val_loss: 0.5846 - val_accuracy: 0.8540\n",
            "Epoch 59/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0987 - accuracy: 0.9624 - val_loss: 0.5417 - val_accuracy: 0.8593\n",
            "Epoch 60/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0986 - accuracy: 0.9626 - val_loss: 0.5644 - val_accuracy: 0.8551\n",
            "Epoch 61/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0990 - accuracy: 0.9620 - val_loss: 0.5800 - val_accuracy: 0.8515\n",
            "Epoch 62/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0952 - accuracy: 0.9638 - val_loss: 0.5723 - val_accuracy: 0.8593\n",
            "Epoch 63/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0973 - accuracy: 0.9630 - val_loss: 0.5517 - val_accuracy: 0.8600\n",
            "Epoch 64/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0971 - accuracy: 0.9633 - val_loss: 0.5581 - val_accuracy: 0.8547\n",
            "Epoch 65/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0958 - accuracy: 0.9640 - val_loss: 0.5685 - val_accuracy: 0.8554\n",
            "Epoch 66/100\n",
            "3125/3125 [==============================] - 17s 5ms/step - loss: 0.0942 - accuracy: 0.9640 - val_loss: 0.6222 - val_accuracy: 0.8487\n",
            "Epoch 67/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0945 - accuracy: 0.9641 - val_loss: 0.6217 - val_accuracy: 0.8426\n",
            "Epoch 68/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0940 - accuracy: 0.9645 - val_loss: 0.5896 - val_accuracy: 0.8537\n",
            "Epoch 69/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0931 - accuracy: 0.9644 - val_loss: 0.5636 - val_accuracy: 0.8549\n",
            "Epoch 70/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0928 - accuracy: 0.9646 - val_loss: 0.5996 - val_accuracy: 0.8562\n",
            "Epoch 71/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0931 - accuracy: 0.9656 - val_loss: 0.5588 - val_accuracy: 0.8579\n",
            "Epoch 72/100\n",
            "3125/3125 [==============================] - 19s 6ms/step - loss: 0.0924 - accuracy: 0.9651 - val_loss: 0.5990 - val_accuracy: 0.8507\n",
            "Epoch 73/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0927 - accuracy: 0.9648 - val_loss: 0.5703 - val_accuracy: 0.8539\n",
            "Epoch 74/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0919 - accuracy: 0.9653 - val_loss: 0.5780 - val_accuracy: 0.8602\n",
            "Epoch 75/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0893 - accuracy: 0.9661 - val_loss: 0.6020 - val_accuracy: 0.8565\n",
            "Epoch 76/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0902 - accuracy: 0.9665 - val_loss: 0.5792 - val_accuracy: 0.8590\n",
            "Epoch 77/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0903 - accuracy: 0.9660 - val_loss: 0.5803 - val_accuracy: 0.8593\n",
            "Epoch 78/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0903 - accuracy: 0.9659 - val_loss: 0.5775 - val_accuracy: 0.8549\n",
            "Epoch 79/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0903 - accuracy: 0.9663 - val_loss: 0.5769 - val_accuracy: 0.8582\n",
            "Epoch 80/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0892 - accuracy: 0.9666 - val_loss: 0.5638 - val_accuracy: 0.8572\n",
            "Epoch 81/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0869 - accuracy: 0.9670 - val_loss: 0.6067 - val_accuracy: 0.8541\n",
            "Epoch 82/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0903 - accuracy: 0.9663 - val_loss: 0.5788 - val_accuracy: 0.8545\n",
            "Epoch 83/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0892 - accuracy: 0.9664 - val_loss: 0.5653 - val_accuracy: 0.8597\n",
            "Epoch 84/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0882 - accuracy: 0.9672 - val_loss: 0.5676 - val_accuracy: 0.8576\n",
            "Epoch 85/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0877 - accuracy: 0.9672 - val_loss: 0.5753 - val_accuracy: 0.8571\n",
            "Epoch 86/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0866 - accuracy: 0.9676 - val_loss: 0.6038 - val_accuracy: 0.8558\n",
            "Epoch 87/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0873 - accuracy: 0.9675 - val_loss: 0.5629 - val_accuracy: 0.8594\n",
            "Epoch 88/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0860 - accuracy: 0.9676 - val_loss: 0.5908 - val_accuracy: 0.8583\n",
            "Epoch 89/100\n",
            "3125/3125 [==============================] - 19s 6ms/step - loss: 0.0862 - accuracy: 0.9676 - val_loss: 0.5734 - val_accuracy: 0.8573\n",
            "Epoch 90/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0849 - accuracy: 0.9683 - val_loss: 0.5767 - val_accuracy: 0.8582\n",
            "Epoch 91/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0851 - accuracy: 0.9682 - val_loss: 0.5945 - val_accuracy: 0.8604\n",
            "Epoch 92/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0861 - accuracy: 0.9675 - val_loss: 0.5979 - val_accuracy: 0.8569\n",
            "Epoch 93/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0850 - accuracy: 0.9682 - val_loss: 0.5675 - val_accuracy: 0.8572\n",
            "Epoch 94/100\n",
            "3125/3125 [==============================] - 17s 6ms/step - loss: 0.0832 - accuracy: 0.9692 - val_loss: 0.5899 - val_accuracy: 0.8566\n",
            "Epoch 95/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0825 - accuracy: 0.9689 - val_loss: 0.6020 - val_accuracy: 0.8581\n",
            "Epoch 96/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0840 - accuracy: 0.9685 - val_loss: 0.6099 - val_accuracy: 0.8601\n",
            "Epoch 97/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0841 - accuracy: 0.9686 - val_loss: 0.5790 - val_accuracy: 0.8587\n",
            "Epoch 98/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0832 - accuracy: 0.9690 - val_loss: 0.6210 - val_accuracy: 0.8555\n",
            "Epoch 99/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0833 - accuracy: 0.9689 - val_loss: 0.5859 - val_accuracy: 0.8563\n",
            "Epoch 100/100\n",
            "3125/3125 [==============================] - 18s 6ms/step - loss: 0.0823 - accuracy: 0.9691 - val_loss: 0.6074 - val_accuracy: 0.8583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff0645392b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REPt4TU9VZ84"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}